// <auto-generated/>
// Do not edit any code this file. Create or edit the partial class instead in a different file.

#nullable enable

using Pulumi.AwsNative.Bedrock.Inputs;

namespace Pulumi.AwsNative.Bedrock;

public static partial class PromptModelInferenceConfigurationArgsExtensions
{
    public static global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs WithMaxTokens(this global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs @selfRef, global::System.Double maxTokens)
    {
        @selfRef.MaxTokens = maxTokens;
        return @selfRef;
    }

    public static global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs WithStopSequences(this global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs @selfRef, global::System.Collections.Generic.List<global::System.String> stopSequences)
    {
        @selfRef.StopSequences = stopSequences;
        return @selfRef;
    }

    public static global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs WithStopSequences(this global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs @selfRef, global::System.Action<global::System.Collections.Generic.List<global::System.String>> @configure)
    {
        var @list = new global::System.Collections.Generic.List<global::System.String>();
        @configure(@list);
        @selfRef.StopSequences = @list;
        return @selfRef;
    }

    public static global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs WithStopSequences(this global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs @selfRef, global::System.Func<global::System.Collections.Generic.IEnumerable<global::System.String>> @create)
    {
        @selfRef.StopSequences = global::Pulumi.Output.Create(@create());
        return @selfRef;
    }

    public static global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs WithTemperature(this global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs @selfRef, global::System.Double temperature)
    {
        @selfRef.Temperature = temperature;
        return @selfRef;
    }

    public static global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs WithTopP(this global::Pulumi.AwsNative.Bedrock.Inputs.PromptModelInferenceConfigurationArgs @selfRef, global::System.Double topP)
    {
        @selfRef.TopP = topP;
        return @selfRef;
    }

}
